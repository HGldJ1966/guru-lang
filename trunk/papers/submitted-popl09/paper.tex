\documentclass[preprint,natbib]{sigplanconf}

%\usepackage{latex8}
%\usepackage{times}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{proof}
\usepackage{url}

% \usepackage{natbib}
%  \bibpunct();A{},
%  \let\cite=\citep

\newcommand{\lam}[2]{\lambda #1 . #2}
\newcommand{\lams}[2]{\lambda^* #1 . #2}
\newcommand{\alet}[3]{\textit{let}\ #1\ :=\ #2\ \textit{in}\ #3}
\newcommand{\Thet}[1]{\lam{V}{\lam{L}{\lam{A}{\lam{O}{\lam{C}{\lam{S}{\lam{D}{#1}}}}}}}}
\newcommand{\ope}[0]{\textit{open}}
\newcommand{\swap}[0]{\textit{swap}}
\newcommand{\vcomp}[0]{\textit{vcomp}}
\newcommand{\nlambda}[0]{\bar{\lambda}}
\newcommand{\nlam}[2]{\nlambda #1 . #2}
\newcommand{\rulename}[1]{\text{\textup{\textsf{#1}}}}
\newcommand\bs{\char '134 }  % A backslash character for \tt font
\newcommand{\seq}[3]{#1 \vdash #2 : #3}
\newcommand{\eval}[0]{\Downarrow}
\newcommand{\evalj}[2]{#1\, \eval\, #2}
\newcommand{\starstar}[0]{*\negthinspace*}
\newcommand{\nat}[0]{\mathbb{N}}
\newcommand{\optt}{\textsc{OpTT}}

\newcommand{\rase}[1]{\ulcorner #1 \urcorner}
\newcommand{\lowr}[1]{\llcorner #1 \lrcorner}

\newcommand{\Eq}[0]{\texttt{=}}
\newcommand{\Neq}[0]{\texttt{!=}}
\newcommand{\Qeq}[0]{\stackrel{?}{=}}
\newcommand{\bang}[0]{\texttt{!}}
\newcommand{\quant}[0]{\textit{Quant}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newcommand{\To}{\Rightarrow}
\newcommand{\gtrans}[2]{#1 \cdot #2}
\newcommand{\gtransa}[2]{#1 \cdot_1 #2}
\newcommand{\gtransb}[2]{#1 \cdot_2 #2}
\newcommand{\gsymm}[1]{#1^{-1}}
\newcommand{\gcong}[1]{f(#1)}
\newcommand{\ginj}[1]{f^{-1}(#1)}

\begin{document}

\conferenceinfo{POPL '09}{\ } 
\copyrightyear{2009} 
\copyrightdata{\ } 

\titlebanner{\ }        % These are ignored unless
\preprintfooter{\ }   % 'preprint' option specified.


%\newenvironment{proof}
%               {\hspace{-1\parindent}\textbf{Proof:}}
%               {\hfill $\square$\vspace{1\baselineskip}}

\title{Verified Programming in Operational Type Theory}

\authorinfo{Aaron Stump}
{CS, The University of Iowa}
{astump@acm.org}

\authorinfo{Morgan Deters}
{LSI, Universitat Polit\`{e}cnica de Catalunya }
{mdeters@cse.wustl.edu}

\authorinfo{Todd Schiller \\ Timothy Simpson \\ Edwin Westbrook} 
{CSE, Washington University in St. Louis}
{\{tws2,tas5,emw1\}@cec.wustl.edu}

%astump@acm.org,mdeters@cse.wustl.edu,tws2@cec.wustl.edu,tas5@cec.wustl.edu,ewestbro@cse.wustl.edu

%\date{}

\maketitle

%\thispagestyle{empty}

\begin{abstract}
An Operational Type Theory (OpTT) is developed based on a
theory of operational joinability of untyped terms.  The theory
accommodates functions which might diverge or abort on some inputs,
while retaining decidability of type checking and logical consistency.
For the latter, OpTT distinguishes proofs from programs, and formulas
from types.  Proofs and other computationally irrelevant annotations,
including data marked as specificational, are dropped during formal
reasoning.  This greatly simplifies verification problems by removing
the need to reason about types when reasoning about programs.  Indeed,
terms with provably disequal types can be shown equal in
OpTT.  OpTT is realized in a tool called
\textsc{Guru}, featuring a type/proof checker and a compiler to C
code.  The theory easily accomodates extensions such as uniqueness and
ownership types, used to track program resources.  Several case
studies are discussed, including ones based on mutable tries.
\end{abstract}

\section{Introduction}

Type theories based on the Curry-Howard isomorphism are of continuing
appeal as providing a unified formalism for writing and verifying
programs.  One well-known drawback is that partial functions must be
accommodated with care, for two reasons.  First, naively allowing
diverging terms renders the logic unsound.  Second, for type theories
with a definitional equality which includes computation, diverging
terms can render type checking undecidable.

The present work is based on the observation that three roles for
evaluation in type theory may be distinguished, which are commonly
unified, particularly under the Curry-Howard isomorphism.  These are
evaluation for program execution, evaluation for proof normalization,
and evaluation for definitional equality.  Distinguishing these roles
for evaluation allows much greater control over the meta-theory of the
system.  By devising different classification (type) systems for
programs, proofs, and their classifiers, we can vary the complexity of
program execution, proof normalization, and definitional equality
independently.  This paper presents one system based on this idea,
called Operational Type Theory (OpTT).  This system allows possibly
diverging or finitely failing programs, while keeping the logic first
order and definitional equality decidable.  The focus here is on
practical program verification.  For formalized mathematics, a
higher-order logic and richer definitional equality might be more
appropriate, and could possibly be developed similarly to \optt.

Having separated proofs and programs, the critical technical challenge
is to allow them nevertheless to interact.  The goal is to support a
combination of internal and external
verification~\cite{alti:esslli96}.  With external verification, proofs
prove specificational statements about the observational behavior of
programs.  With internal verification, specifications are expressed
through rich typing of the programs, typically using dependent
function types and indexed datatypes.  Each style has its advantages
and disadvantages: the former is more flexible, since additional
properties can be proved after coding without modifying the program;
while the latter is closer to existing programming practice.  The
combination of internal and external verification leads to technical
puzzles, such as how to state equalities between terms with provably
but not definitionally equal types~\cite{hofmann+98}.  One answer is
heterogeneous equality, another is contextual definitional
equality~\cite{blanqui+08,mcbride99}.

\optt\ combines internal and external verification in a different way
by taking an untyped propositional equality on type-free terms, with
all typing annotations dropped.  Examples of such annotations are
proofs in explicit casts, which are used to establish type
equivalences beyond definitional equality; and also data marked as
specificational.  Such data have been dropped in previous work on
compilation of dependently typed programs~\cite{brady+03}.  \optt\
takes this a step further, by allowing external reasoning in the
theory about such type-free terms.  The practical benefits of this are
significant, since it is no longer necessary to reason about the types
of terms when reasoning about the terms.  Indeed, terms can be proved
equal which have provably disequal types.  An example of this is given
in Section~\ref{sec:eg2} below.  Provable equality soundly captures
joinability of untyped terms in the call-by-value operational
semantics of the language, thus giving \optt\ its name.

\textbf{Primary Contributions.} The central design idea developed in
\optt\ is its untyped operational equality, based on a principled
separation of proofs and programs.  The latter allows support for
diverging terms while keeping a consistent logic and decidable
definitional equality.  Dropping annotations when reasoning about
terms -- indeed, as part of definitional equality -- turns out to
provide a very robust basis for extensions to the theory.  Extensions
discussed in this paper include termination casts (like type casts but
used to show the type checker that a term is terminating); and
ownership and uniqueness annotations, which allow sound functional
modeling of non-functional operations like destructive updates and
input/output.  \optt\ with these extensions has been implemented in a
tool called \textsc{Guru}, including a type/proof checker and a
compiler to C.  Using this tool, several examples are presented which
are challenging or impossible for other type theories
(Section~\ref{sec:eg}).  The meta-theory of \optt\ is considered, in
particular cut elimination and type soundness
(Sections~\ref{sec:cut_elim} and~\ref{sec:typesound}).  A larger case
study is described (Section~\ref{sec:golfsock}), namely an
implementation called \textsc{Golfsock} of an optimized type checker
for the Edinburgh Logical Framework~\cite{HHP93}.  We begin by
considering additional related work, and then present the syntax,
typing rules, and operational semantics of terms
(Section~\ref{sec:terms}).  \textsc{Guru} and the code and proofs for
the examples are freely available at \url{cl.cse.wustl.edu/guru/}.

\section{Related Work}

\textbf{Intensional Type Theory.} One approach to accommodating
general recursive programs in intensional type theory is to require
such programs to take an additional input, which restricts the program
to a subset of its nominal domain on which it is uniformly terminating
(see, e.g.,~\cite{bove+05}).  Programs which truly might fail to
terminate are not allowed, and finite failure is usually not
considered.  Finite failure simplifies code on inputs outside the
intended domain, and is supported by all practical programming
languages.  Another approach views potentially non-terminating
computations as elements of a co-inductive type, and combines such
computations monadically~\cite{capretta05}.  This method accommodates
general computations indirectly, and requires co-inductive types.  In
contrast, the present work provides direct support for general
computations, and does not rely on co-inductive types.

\textbf{Extensional Type Theory.} In~\cite{constable+89} and
consequent literature, the \textsc{Nuprl} type theory is extended to
accommodate partial functions via liftings $\bar{A}$ of total types
$A$.  Possibly diverging terms may inhabit $\bar{A}$.  Since
\textsc{Nuprl} has an undecidable type checking problem, the technical
problems encountered are different than for intensional theories.
Previous work adding lifted types to the Calculus of Constructions
sacrificed decidability of type checking~\cite{audebaud91}.  The
recently proposed Observational Type Theory (\texttt{OTT}) supports
extensionality while retaining decidable type
checking~\cite{altenkirch+07}.  \texttt{OTT} cannot directly
accommodate truly non-terminating functions, however, and has not yet
been extended with co-inductive types.

\textbf{Logics of Partial Terms.} Logics of partial terms support
reasoning about termination and non-termination of partial recursive
functions~\cite{stark98,B85}.  These systems are not based on the
Curry-Howard isomorphism and hence do not suffer from the problems
associated with supporting partial functions in type theory.  On the
other hand, they lack the expressiveness and conceptual unity of type
theory for writing and verifying typed programs.

\textbf{Dependent Types for Programming.} \textsc{Epigram} is a total
type theory proposed for practical programming with dependent
types~\cite{mcbride+04}.  Xi's \textsc{ATS} and a system proposed by
Licata and Harper have similar aims, but allow general
recursion~\cite{licata+05,chenxi05}.  Programs which can index types
and which are subject to external reasoning, however, are required to
be uniformly terminating.  This is done via \emph{stratification}:
such terms are drawn from a syntactic class distinct from that of
program terms.  Existing stratified systems restrict external
verification to terms in the index domain.  Similar approaches are
taken in \textsc{Concoqtion} and
$\Omega$\textsc{mega}~\cite{pasalic+07,sheard06}.  Hoare Type Theory
supports internal verification of possibly non-terminating, imperative
programs, but at present does not support external verification of
such programs~\cite{nanevski+05}.

\textbf{DFOL.} A final piece of related work develops a dependently
typed first-order logic (DFOL)~\cite{rabe06}.  The logic of \optt\
improves upon this system by allowing both term constructors and
computational functions to accept proofs as arguments, while remaining
first-order (in the sense of normalization complexity).

\section{Terms and Types}
\label{sec:terms}

The syntax for \optt\ terms and types is given in
Figure~\ref{fig:terms}.  The syntax-directed classification rules for
terms are given in Figure~\ref{fig:cl-term}, with those for types and
kinds omitted for space reasons.  All classification rules in this
paper compute a classifier as output for a given context $\Gamma$ and
expression as input.  The rules operate modulo definitional equality,
defined below.  The syntax-directed nature of these classification
rules, together with the proof rules presented subsequently, imply
decidability of classification.  A few points are needed before
turning to an overview of the constructs.

\textbf{Meta-variables.} We write $P$ for
proofs, and $F$ for formulas, defined in Section~\ref{sec:pfs}.  We
use $x$ for variables, $c$ for term constructors, and $d$ for type
constructors.  We occasionally use $v$ for any variable or term
constructor.  Variables are considered to be syntactically
distinguished as either term-, type-, or proof-level.  This enables
definitional equality to recognize which variables are proofs or
types.  A reserved constant $\bang$ is used for erased annotations,
including types (Section~\ref{sec:defeq}).  

\textbf{Specificationality.} We write $o$ for ownership annotations on
function inputs.  These will be extended below, but for now, the only
such is \texttt{spec}, for specificationality.  Similarly, $s$ is for
indicating specificationality of arguments in applications: either
\texttt{spec} or nothing.  The reason for marking specificationality
syntactically is to keep definitional equality from depending on
typing.  A simple static \emph{specificationality analysis}, not
described here for space reasons, ensures that specificational inputs
are used only in specificational arguments.  This is performed as a
separate analysis from typing, and so the typing rules ignore
specificationality annotations.  In the \textsc{Guru} implementation,
the programmer explicitly marks function inputs as specificational,
while the specificationality annotations on arguments are inferred
during type checking.

\textbf{Multi-arity notations.} We write $\texttt{fun}\
x(\bar{o}\ \bar{x}\,:\,\bar{A}) : T .\ t$ for $\texttt{fun}\ x(o_1\
x_1\,:\,A_1)\cdots(o_n\ x_n\, :\,A_n) : T .\ t$, with $n > 0$, and
each $o_i$ either \texttt{spec} or nothing.  Also, in the \texttt{fun}
typing rule, we use judgment $\seq{\Gamma}{\bar{x}}{\bar{A}}$:
\[ 
\begin{array}{ll}
\infer{\seq{\Gamma}{x,\bar{x}}{A,\bar{A}}}
      {\seq{\Gamma}{A}{\textit{sort}(A)} & \seq{\Gamma,x:A}{\bar{x}}{\bar{A}}}
&
\infer{\seq{\Gamma}{\cdot}{\cdot}}
      {\ }
\end{array}
\]
Here and in several other rules, $\textit{sort}$ is a meta-level
function assigning a sort to every expression.  The possible sort of a
type is \texttt{type}, of \texttt{type} is \texttt{kind}, and of a
formula is \texttt{formula}.

\textbf{The \texttt{Terminates} judgment.} Specificational arguments
are required to be terminating, using a \texttt{Terminates} judgment.
This is also used in quantifier proof rules below.  Terminating terms
here are just variables, constructors, and applications of
constructors to terminating terms.  Types and proofs are also
terminating.  We omit the simple rules for the \texttt{Terminates}
judgment.  The class of terms which \texttt{Terminates} recognizes as
terminating is expanded in Section~\ref{sec:terminates} below.

\textbf{Conditions on \texttt{match}.} The \texttt{match} typing rule
has one premise for each case of the match expression (indicated using
meta-level bounded universal quantification in the premise).  The
premise requiring $T$ to be a type is to ensure it does not contain
free pattern variables.  The rule also has several conditions not
expressed in the figure.  First, the term constructors $c_1, \cdots,
c_n$ are all and only those of the type constructor $d$, and $n$ must
be at least one (\texttt{match}es with no cases are problematic for
type computation without an additional annotation).  Second, the
context $\Delta_i$ is the one assigning to pairwise distinct variables
$\bar{x}_i$ the types required by the declaration of the constructor
$c_i$.  Third, the type $T_i$ is the return type for constructor
$c_i$, where the pattern variables have been substituted for the input
variables of $c_i$.  Fourth, the type constructor is allowed to be
$0$-ary, in which case $\langle d\ \bar{X}\rangle$ should be
interpreted here as just $d$.  The uninformative formalization of
these conditions is omitted.

\begin{figure}
\begin{eqnarray*}
t & ::= & x\ ||\ c\ ||\ 
       \texttt{fun}\ x(\bar{o}\ \bar{x}\,:\,\bar{A}) : T .\ t\ ||\ (t\ s\ X)\ ||
\\ 
\ &\ &        \texttt{cast}\ t\ \texttt{by}\ P  \ ||\ \texttt{abort}\ T\ ||\\ 
\ & \ & \texttt{let}\ x\ \Eq\ t\ \texttt{by}\ y\ \texttt{in}\ t' \ ||\ 
\\ 
\ & \ & \texttt{match}\ t\ \texttt{by}\ x\ y \ \texttt{with}\\
\ & \ & \ \ \ c_1\ \bar{s}_1\ \bar{x}_1\ \texttt{=>}\ t_1 |
 \cdots | c_n\ \bar{s}_n\ \bar{x}_n\ \texttt{=>}\ t_n\ \texttt{end}\ ||\\
\ & \ & \texttt{existse\_term}\ P\ t
\\
\\
X & ::= & t ||\ T\ ||\ P
\\
\\
A & ::= & T\ ||\ \texttt{type}\ ||\ F 
\\
\\
T & ::= & x\ ||\ d\ ||\ \bang\ ||\ 
     \texttt{Fun}(o\ x\,:\,A) . \ T\ ||\ \langle T\ Y\rangle 
\\
\\
Y & ::= & t\ ||\ T
\end{eqnarray*}
\caption{\label{fig:terms} Terms ($t$) and Types ($T$)}
\end{figure}


\begin{figure}
\begin{tabular}{l}
\begin{tabular}{ll}
\infer{\seq{\Gamma}{v}{A}}{\Gamma(v) = A}

&
\infer{\seq{\Gamma}{\texttt{abort}\ T}{T}}{\seq{\Gamma}{T}{\texttt{type}}}

\end{tabular}
\\
\\


\infer{
\seq{\Gamma}{\texttt{fun}\ x(\bar{o}\ \bar{x}\,:\,\bar{A}):T.\ t}
            {\texttt{Fun}(\bar{o}\ \bar{x}\,:\,\bar{A}).\ T} }
{\begin{array}{l}
x,\bar{x}\not\in\textit{FV}(T) \\
\seq{\Gamma}{\bar{x}}{\bar{A}} \ \ \ \  \seq{\Gamma, \bar{x}:\bar{A},x\,:\,\texttt{Fun}(\bar{x}\,:\,\bar{A}).\ T}{t}{T}
\end{array}}

\\
\\

\infer{\seq{\Gamma}{(t\ s\ X)}{[X/x]T}}
      {
\begin{array}{l}
\textnormal{if } s = \texttt{spec}\textnormal{, then }\texttt{Terminates}\ X \\
\seq{\Gamma}{t}{\texttt{Fun}(x : A).\ T} \ \ \ \ \seq{\Gamma}{X}{A}
\end{array}} 

\\
\\

\infer{\seq{\Gamma}{\texttt{cast}\ t\ \texttt{by}\ P}{T_2}}
      {\seq{\Gamma}{t}{T_1} & 
        \seq{\Gamma}{P}{\{T_1 = T_2\}}}

\\
\\

\infer{
\seq{\Gamma}{\texttt{let}\ x\ \Eq\ t\ \texttt{by}\ y\ \texttt{in}\ t'}{T}}
{\seq{\Gamma}{t}{A} &
\seq{\Gamma,x:A,y:\{x\, \Eq\, t\}}{t'}{T} & x,y\not\in\textit{FV}(T)}

\\
\\

\infer{
\begin{array}{l}
\Gamma \vdash \texttt{match}\ t\ \texttt{by}\ x\,y\ \ 
\texttt{with} \\ 
\ \ \ \ \ \ \  c_1\ \bar{s}_1\ \bar{x}_1 \texttt{=>} s_1 | \ldots 
        | c_n\ \bar{s}_n\ \bar{x}_n \texttt{=>} s_n\ \texttt{end}\ :\ T
\end{array}}
      {\begin{array}{l}
       \seq{\Gamma}{t}{\langle d\ \bar{X}\rangle}\ \ \ \ \  \seq{\Gamma}{T}{\texttt{type}}
    \\ \forall\ i \le n.\\ (\seq{\Gamma,\Delta_i,x:\{t\, \Eq\, (c_i\ \bar{x}_i)\},
y:\{\langle d\ \bar{X}\rangle\, \Eq\, T_i\}}{s_i}{T})
\end{array}}
     
\\
\\

\infer{\seq{\Gamma}{\texttt{existse\_term}\ P\ t}{C}}
{\begin{array}{ll}
\seq{\Gamma}{P}{\texttt{Exists}(x : A). \hat{F}[x]} &\ \\
\seq{\Gamma}{t}{\texttt{Fun}(\texttt{spec}\ x : A)(u : \hat{F}[x]). C & x,u\not\in\textit{FV}(C)}
\end{array}}

\end{tabular}
\caption{\label{fig:cl-term}Term Classification}
\end{figure}

\subsection{Overview of Constructs}

Our \texttt{cast}-terms witness to the type checker that a term may be
viewed as having an equal type.  For the benefit of \texttt{casts},
\texttt{match}-terms bind variables for assumptions of equalities in
the cases.  Specifically, assumption variables $x$ and $y$ are bound
(just) in the bodies of the cases, and serve as assumptions of
different equalities in each case: the former that the scrutinee
equals the pattern, and the latter that the scrutinee's type equals
the pattern's type.  The same assumption variables ($x$ and $y$) are
assigned different classifiers in the different cases.  

In \texttt{fun}-abstractions, the bound variable $x$ immediately
following the \texttt{fun} keyword can be used for recursive calls in
the body of the abstraction.  It may be omitted, in which case the
type annotation following the \texttt{fun}-term's declarations can
also be omitted.  Recursive multi-arity \texttt{fun}-terms as
described in Figure~\ref{fig:terms} cannot always be translated into
nested unary \texttt{fun}-terms, due to the dependent typing.  The
\texttt{abort} term cancels all pending evaluation.  It is annotated
with a type to facilitate type computation.  A related construct, not
otherwise mentioned, is $\texttt{impossible}\ P\ T$.  This is
definitionally equal to an \texttt{abort}, but documents via a proof
$P$ of a contradiction that execution cannot reach this point.

Our \texttt{let}-terms are as usual, except that like
\texttt{match}-terms, they also bind an assumption variable.  In a
\texttt{let}-term, the variable $y$, bound in the body of the
\texttt{let}-term, serves as an assumption of the equality ${x\, \Eq\,
t}$.  \textsc{Guru} includes a mechanism for local macro definitions
at the term, type, formula, and proof levels, not presented here for
space reasons.

Finally, to make use of proved existentials in code, there is a
term-level existential elimination, \texttt{existse\_term}.  This is
similar to proof-level existential elimination (see
Section~\ref{sec:pfs}), but it passes the element proven to exist to a
term-level function as a specificational argument.  Marking the
argument as specificational ensures that computation cannot depend on
the value of this element.


\subsection{Definitional Equality}
\label{sec:defeq}

Proofs, type annotations, and specificational data are of interest
only for type checking, and are dropped during evaluation.  Our
definitional equality takes this into account.  It also takes into
account safe renaming of variables, and replacement of defined
constants by the terms they are defined to equal.  Flattening of
left-nested applications, and right-nested \texttt{fun}-terms and
\texttt{Fun}-types is also included.  More formally, definitional
equality is the least congruence relation which makes (terms or types)
$Y \approx Y'$ when any of these conditions holds:
\begin{enumerate}
\item $Y =_\alpha Y'$ ($Y$ and $Y'$ are identical modulo safe renaming
of bound variables).
\item $Y \equiv \hat{Y}[c]$ and $Y' \equiv \hat{Y}[t_c]$, where $c$ is
defined non-recursively at the top level to equal $t_c$ (see
Section~\ref{sec:commands} below).
\item Nested applications and abstractions in $Y$ and $Y'$ flatten
to the same result, as mentioned above.
\item $Y \To Y'$, using the first-order term rewriting system of
Figure~\ref{fig:dropannos} (where we temporarily view abstractions
as first-order terms).
\end{enumerate}

\noindent The rules of Figure~\ref{fig:dropannos} drop annotations in
favor of the special constant $\bang$, mentioned above.  There, we
temporarily write $P^-$ for a proof $P$ which is not \bang, and
similarly for $T^-$ and $A^-$.  The rules also operate on members of
the list of input declarations in a \texttt{fun}-term, as first class
expressions.  Such lists can be emptied by dropping specificational
inputs (hence the first rule in the figure).  We temporarily consider
patterns in \texttt{match} terms as applications, and hence apply the
rules for rewriting applications to them.  The rules are locally
confluent and terminating, so we can define a function $|\cdot|$ to
return the unique normal form of any expression under the rules.
Notice that types are dropped only where used in terms.  So $|T|$ is
not $\bang$ for any type $T$.  Note that dropping annotations is
defined on both typeful and type-free expressions.

\begin{figure}
\[
\begin{array}{lll} 
\texttt{fun}\ x()\, :\, T\, .\ t & \To & t \\
\texttt{fun}\ x(\bar{x}\,:\,\bar{A})\, :\, T^-\, .\ t & \To &
\texttt{fun}\ x(\bar{x}\,:\,\bar{A})\, :\, \bang\, .\ t \\
P^- & \To & \bang \\
(t\ T^-) & \To & (t\ \bang) \\
(t\ \texttt{spec}\ X) & \To & (t\ \bang) \\
(t\ \bang) & \To & t \\
\texttt{cast}\ t\ \texttt{by}\ P & \To & t \\
\texttt{abort}\ T^- & \To & \texttt{abort}\ \bang \\
\texttt{existse\_term}\ P\ t & \To & t \\
(\bar{x}\,:\,\bar{A}-) & \To & (\bar{x}\,:\,\bang) \\
(\texttt{spec}\ \bar{x}\,:\,\bar{A}-) & \To & \cdot
\end{array}
\]
\caption{\label{fig:dropannos}Dropping Annotations}
\end{figure}

Definitional equality is easily decided by, for example, considering
the \emph{unannotated expansions} of the expressions in question.
These expansions result from replacing all constants with their
definitions, then dropping all annotations, and then putting terms
into an $\alpha$-canonical form.

The distinction between terms, types, proofs, and formulas provides a
simple principled basis for adopting different definitional equalities
in different settings.  Definitional equality as just defined we term
\emph{computational} definitional equality, and use it when
classifying terms not inside types, proofs, or formulas.  We also
define a \emph{specificational} definitional equality, used in all
other situations.  The difference at this point in our development is
just that specificational equality drops specificationality
annotations from \texttt{Fun}-types:
\[
\begin{array}{lll} 
\texttt{Fun}(\texttt{spec}\ x:A).\,T & \To & 
\texttt{Fun}(x:A).\,T 
\end{array}
\]

\noindent These annotations are relevant only for type checking terms
and specificationality analysis, and not for reasoning.  Dropping them
during formal reasoning avoids some clutter in proofs, since theorems
need not mention specificationality.  We will put the distinction
between computational and specificational disequality to work more
crucially when we consider functional modeling in
Section~\ref{sec:model} below.

\subsection{Operational Semantics}
\label{sec:opsem}

Evaluation in \optt\ is call-by-value.  We omit the straightforward
definition of the small-step evaluation relation $\leadsto$ for space
reasons.  Note only that it is defined just on terms with annotations
dropped.  Also, for the benefit of proof rules below, we take
$\leadsto$ to be small-step partial evaluation, where free
(term-level) variables are considered to be values.  Such variables
can be introduced by the universal introduction proof rule or
induction proof rule.

\subsection{Type Refinement}

In principle, the assumption variables provided by
\texttt{match}-terms are sufficient for building proofs needed to
refine the types of terms (by casting) in cases.  In practice, while
automation cannot perform all necessary refinements (due to
undecidability of type inhabitation in the presence of indexed
datatypes), some automation can alleviate the burden of casts
significantly.  \textsc{Guru} implements a simple form of type
refinement using first-order matching (modulo definitional equality)
of the type of the pattern and the type of the scrutinee.  Impossible
cases are detected by match failure.  Systems with richer definitional
equality still need casts in general, at which point they can become
problematic for external reasoning.

\subsection{Commands and Datatypes}
\label{sec:commands}

We avoid the uninformative formalization of a typing signature
declaring and defining constants, and instead adopt an implementor's
perspective on declarations and definitions.  Input to an
implementation of \optt\ is a sequence of commands, with syntax
described in Figure~\ref{fig:command}.  Here, $G$ ranges over terms,
types and type families, formulas, and proofs (defined in
Section~\ref{sec:terms} above and~\ref{sec:pfs} below).  $K$ is for
kinds.  Type and term constructors are introduced by the
\texttt{Inductive} command, and defined constants by the
\texttt{Define} command.  Datatypes may be both term- and
type-indexed.  We additionally restrict input types to a (term)
constructor for $d$ so that they may contain $d$ nested in type
applications, but not in \texttt{Fun}-types or formulas.  The syntax
also prohibits type constructors from accepting proofs as arguments.

\begin{figure}
\[
\begin{array}{l} 
\texttt{Define}\ c\, :\, A\ :=\ G. \\ \\
\texttt{Inductive}\ d\, :\, K \ := \ c_1 : D_1\ |\ \ldots\ |\ c_k : D_k\, .
\\ \\ 
\textit{where}\\
\begin{array}{lll}
D & ::= & \texttt{Fun}(\bar{y}\,:\,\bar{A}) . \langle  d\ Y_1\ \ldots\ Y_n\rangle
\\
\\
K & ::= & \texttt{type}\ ||\ \texttt{Fun}(x\,:\,B).\ K
\\
\\ 
B & ::= & \texttt{type}\ ||\ T
\end{array}
\end{array}
\]
\caption{\label{fig:command}Commands}
\end{figure}

\section{Proofs and Formulas}
\label{sec:pfs}

The syntax of \optt\ formulas and selected proof constructs is given
in Figure~\ref{fig:formulas-proofs}.  For compact notation, we view
implications as degenerate forms of universal quantifications, and
similarly conjunctions as existential quantifications.  We find we do
not need disjunction (not to be confused with the boolean \texttt{or}
operation) for any of a broad range of program verification examples,
so we exclude it for simplicity.  The syntax-directed classification
rules are given in Figures~\ref{fig:cl-formula},~\ref{fig:cl-proof},
and~\ref{fig:cl-proof2}.  Classification of proofs and formulas, as of
terms and types, is easily seen to be decidable.  Note that proofs may
contain term- and type-level variables, so the first rule of
Figure~\ref{fig:cl-proof} is indeed needed.  There are several
additional points to mention:

\textbf{Untyped equations.} Equations and disequations are formed
between type-free terms, as well as types.  Instead of allowing any
untyped terms, one could require some form of approximate typing, but
this is not essential nor required in practice.  In addition to the
equational principles mentioned here, there are injectivity principles
for term and type constructors, omitted for space reasons.  In
Section~\ref{sec:eqext}, we consider several additional equational
principles.

\textbf{Quantified formulas.} The first rule for classifying
quantified formulas disallows the body of such a formula from
depending on the bound variable, if the bound variable ranges over
proofs of formulas.  This just enforces that implications are not
dependently typed, and similarly for conjunctions, even though they
are written for compactness using \texttt{Forall} and \texttt{Exists}.

\textbf{Evaluation.} The rule \texttt{evalstep} axiomatizes the
small-step operational semantics.  In practice, as in other theorem
provers, higher-level tactics are needed in practice.  \textsc{Guru}
implements several of these, discussed below.

\textbf{Contexts.} All contexts must have at least one occurrence of
the primary hole $*$.  Insertion of an expression into a hole is
capture-avoiding.  This disallows truly extensional reasoning about
\texttt{fun}-terms and \texttt{Fun}-types (discussed further in
Section~\ref{sec:eqext} below).  The context $\hat{F}$ used in the
syntax for \texttt{existsi} is just a formula with a hole indicated by
$*$.  Contexts $Y^+$ for equational congruence reasoning
(\texttt{cong}) allow replacements to take place everywhere in terms
and types.  At the term level, this is more liberal than replacement
using the evaluation contexts of the operational semantics.  The more
liberal policy is very convenient when writing proofs in \optt, since
it allows replacement of arguments in function calls, which would not
be directly allowed using contexts $E$.  Despite this more liberal
policy, equality is intensional (see Section~\ref{sec:eqcuts}).

\textbf{\texttt{Terminates} and Quantifiers.} \texttt{Forall}-elimination
and \texttt{Exists}-introduction require the instantiating and
witnessing terms, respectively, to be typed terminating expressions.
Quantifiers in \optt\ range over values (excluding non-terminating
terms), and hence this restriction is required for soundness.

\textbf{Induction.} Classification for \texttt{induction}-proofs is
not stated in the figure, for space reasons.  These proofs are similar
to a combination of recursive \texttt{fun}-terms and
\texttt{match}-terms.  A third assumption variable is bound in the
cases, for the induction hypothesis.  The last classifier in the list
$\bar{B}$ (see Figure~\ref{fig:formulas-proofs}) is required to be a
datatype (i.e., of the form $\langle d\ \bar{Y}\rangle$).  The last
variable in the list is thus the parameter of induction.  Earlier
parameters may be needed due to dependent typing.  The classifier for
the proof is then of the form $\texttt{Forall}(\bar{x}\,:\,\bar{A}). F$.

\begin{figure}
\begin{eqnarray*}
F & ::= & \quant(x\,:\,A). \, F\ ||\  \{ Y_1\, \Qeq\, Y_2 \} \\ \\
\quant & \in & \{ \texttt{Forall}, \texttt{Exists} \} \\ 
\Qeq & \in & \{ \Eq, \Neq \} 
\\ \\
P & ::= & x\ ||\ \bang\ ||\ 
        \texttt{foralli}
         (x\,:\,A) . \, P\ ||\ [P\ X_1 \cdots X_n]\ || \\ 
\ & \ &  \texttt{existsi}\ t\ \hat{F}\ P\ ||\ \texttt{existse}\ P\ P'\ || \\
\ & \ &  \texttt{join}_{n,m}\ t\ t' \ ||\
 \texttt{symm}\ P\ ||\ \texttt{trans}\ P\ P'\ ||\\
\ & \ & \texttt{cong}\ Y^+\ \bar{P}\ || \ \texttt{contra}\ P\ F\ ||\ \cdots \ || \\
\ & \ & \texttt{induction}(\bar{x}\,:\,\bar{A})\\
\ & \ & \ \ \  \texttt{by}\ x\ y\ z\ 
\texttt{return}\ F\ \texttt{with}\\
\ & \ & \ \ \  
c_1\ \bar{x}_1\ \texttt{=>}\ P_1 | 
 \cdots | c_n\ \bar{x}_n\ \texttt{=>}\ P_n\ \texttt{end}
\end{eqnarray*}
\caption{\label{fig:formulas-proofs} Formulas ($F$) and Proofs ($P$)}
\end{figure}

\begin{figure}
\begin{tabular}{l}
\infer{
\seq{\Gamma}{\quant(x\,:\,F_1).\ F_2}{\texttt{formula}} }
{\seq{\Gamma}{F_1}{\texttt{formula}} & \seq{\Gamma}{F_2}{\texttt{formula}}}

\\ 
\\ 
\infer{
\seq{\Gamma}{\quant(x\,:\,B).\ F}{\texttt{formula}} }
{\seq{\Gamma}{T}{\texttt{type}} & \seq{\Gamma, x\,:\,T}{F}{\texttt{formula}}}

\\ 
\\ 

\infer{
\seq{\Gamma}{\{t_1\, \Qeq\, t_2\}}{\texttt{formula}}}
{\ }
\\ 
\\ 

\infer{
\seq{\Gamma}{\{T_1\, \Qeq\, T_2\}}{\texttt{formula}}}
{\ }

\end{tabular}
\caption{\label{fig:cl-formula}Formula Classification}
\end{figure}

\begin{figure}
\begin{tabular}{l}
\infer{\seq{\Gamma}{x}{A}}{\Gamma(x) = A}

\\
\\

\infer{
\seq{\Gamma}{\texttt{foralli}(x\,:\,A).\ P}{\texttt{Forall}(x\,:\,A).\ F} }
{\seq{\Gamma}{A}{\textit{sort}(A)} & \seq{\Gamma, x\,:\,A}{P}{F}}

\\
\\

\infer{
\seq{\Gamma}{[P\ X]}{[X/x] F} }
{\seq{\Gamma}{P}{\texttt{Forall}(x:A).\ F} & \seq{\Gamma}{X}{A} & \texttt{Terminates}\ X}

\\
\\

\infer{
\seq{\Gamma}{\texttt{existsi}\ X\ \hat{F}\ P}{\texttt{Exists}(x : A) .
 \hat{F}[x]} }
{\seq{\Gamma}{P}{\hat{F}[X]} & \seq{\Gamma}{X}{A} & \texttt{Terminates}\ X}

\\
\\

\infer{
\seq{\Gamma}{\texttt{existse}\ P\ P'}{C}}
{\begin{array}{ll}
\seq{\Gamma}{P}{\texttt{Exists}(x : A). \hat{F}[x] } &\ \\
\seq{\Gamma}{P'}{\texttt{Forall}(x : A)(u : \hat{F}[x]). C & x,u\not\in\textit{FV}(C)}
\end{array}}

\\ \\
\infer{
\seq{\Gamma}{\texttt{contra}\ P\ F}{F}
 }
{\seq{\Gamma}{P}{\{ Y\ \Neq\ Y \} } & \seq{\Gamma}{F}{\texttt{formula}}}


\end{tabular}
\caption{\label{fig:cl-proof}Logical Inferences}
\end{figure}

\begin{figure}
\begin{tabular}{l}

\infer{
\seq{\Gamma}{\texttt{evalstep}\ t}{\{t\ \Eq\ t'\}} }
{|t|\ \leadsto\ t'}

\\
\\

\infer{
\seq{\Gamma}{\texttt{symm}\ P}{\{Y'\ \Qeq\ Y\}} }
{\seq{\Gamma}{P}{\{Y\ \Qeq\ Y'\}}}

\\
\\

\infer{
\seq{\Gamma}{\texttt{trans}\ P_1\ P_2}{\{Y_1\ \Qeq\ Y_3\}} }
{\seq{\Gamma}{P_1}{\{Y_1\ \Eq\ Y_2\}} & \seq{\Gamma}{P_2}{\{Y_2\ \Qeq\ Y_3\}} }

\\
\\

\infer{
\seq{\Gamma}{\texttt{cong}\ Y^+\ P}{\{Y^+[Y]\ \Eq\ Y^+[Y']\}} }
{\seq{\Gamma}{P}{\{Y\ \Eq\ Y'\}}}

\end{tabular}
\caption{\label{fig:cl-proof2}Example Equational Inferences}
\end{figure}

\subsection{Evaluation Tactics}

While the core proof language just described is sufficient in theory,
in practice one needs tactics, as in other theorem provers.  Theorem
provers often provide incomplete or non-terminating tactics, though
of course not unsound ones.  \textsc{Guru} implements several tactics
for equational reasoning.  The simplest is \texttt{eval}, for evaluating
a term to a normal form:
\[
\infer{
\seq{\Gamma}{\texttt{eval}\ t}{\{t\ \Eq\ t'\}} }
{|t|\ \leadsto^{!}\ t'}
\]

\noindent Similarly, we have \texttt{evalto}, which may stop before
a normal form is reached:
\[
\infer{
\seq{\Gamma}{\texttt{evalto}\ t\ t'}{\{t\ \Eq\ t'\}} }
{|t|\ \leadsto^{*}\ t'}
\]

\noindent Most frequently used in practice is \texttt{join}, for
joining terms at a normal form:
\[
\infer{
\seq{\Gamma}{\texttt{join}\ t\ t'}{\{t\ \Eq\ t'\}} }
{|t|\ \leadsto^{!}\ t''\ &\ |t''|\ \leadsto^{!}\ t''}
\]

\noindent The most sophisticated tactic, developed in Adam Petcher's
Master's thesis, is \texttt{hypjoin}~\cite{petcher08}.  This is like
\texttt{join}, but operates modulo a set of proved (possibly
non-normalized) ground equations, given as inputs to the tactic by the
user.  The tactic essentially implements congruence closure modulo
evaluation.

All these tactics rely on undecidable side conditions about
evaluation, and may fail to terminate.  If they terminate, however, a
proof using just \texttt{evalstep} and basic equational reasoning
principles can in principle be reconstructed, though this is not
currently implemented in \textsc{Guru}.  

\section{Simple Examples}
\label{sec:eg}

We consider here three simple examples, which have been machine
checked using the \textsc{Guru} implementation of \optt.  The first
could be done in type theories like Coq's, albeit with more work (in a
precise sense described below).  The second and third cannot, for
different reasons, be done in type theories like Coq's.  A larger case
study is discussed in a subsequent section.

\subsection{Associativity of Append on Lists with Length}

Internally verifying that the length of appended lists is equal to the
sum of their lengths is standard for dependently typed programming.
Externally verifying associativity of such a function is not.  Such
reasoning is possible in systems like \textsc{Coq}, but generally
requires the use of an additional axiom expresssing some form of proof
irrelevance~\cite{hofmann+98,coq}.  We first declare a datatype of
lists with length, assuming a standard definition of the datatype
\texttt{nat} for unary natural numbers.  The type \texttt{<vec A n>}
is inhabited by all and only (finite) lists of elements of type
\texttt{A} of length \texttt{n}.  The \texttt{vecn} constructor
creates a list of length zero (``\texttt{Z}''), and \texttt{vecc} one
of length \texttt{(S n)} from a list of length \texttt{n}.

\begin{verbatim}
Inductive vec : Fun(A:type)(n:nat). type :=
  vecn : Fun(A:type). <vec A Z>
| vecc : Fun(A:type)(n:nat)(a:A)
             (l:<vec A n>).<vec A (S n)>.
\end{verbatim}

\noindent
We may now define a recursive function \texttt{append} with the
following type.  This type states that the length of the output list
is the sum (assuming a standard definition of \texttt{plus}) of the
lengths of the input lists.  It also records that the lengths are
specificational data.  Specificational data analysis enforces
statically that computational results cannot depend on these
lengths, but only the lists themselves.

\begin{verbatim}
Fun(A:type)(spec n m:nat)
   (l1 : <vec A n>)(l2 : <vec A m>).
   <vec A (plus n m)> 
\end{verbatim}


\noindent The code for \texttt{append} uses casts in several places.
For example, the body of \texttt{append} is as follows, where
\texttt{P1} and \texttt{P2} are short equational proofs omitted here:

\begin{verbatim}
match l1 by u v with
  vecn A' => cast l2 by P1
| vecc A' n' x l1' => 
  cast
    (vecc A' (plus n' m) x 
      (vec_append A' n' m l1' l2)) 
  by P2

\end{verbatim}
             
\noindent The issue is that \texttt{l2} has type \texttt{<vec A m>},
but the expected return type of the function is \texttt{<vec A (plus n
m)>}.  So in each case, a cast is used with a proof \texttt{P1} or
\texttt{P2}, which shows that \texttt{\{ m = (plus n m)\}}.  These
proofs uses the assumption \texttt{v} that the type of \texttt{l1},
namely \texttt{<vec A n>}, is equal to the type of the pattern, namely
\texttt{<vec A Z>} in the first case, and \texttt{<vec A (S n')>} in
the second.  From these, using injectivity of \texttt{vec}, we can
derive \texttt{\{n = Z\}} and \texttt{\{n = (S n')\}}, respectively;
from which the equation between \texttt{n} and \texttt{(plus n m)}
follows in each case.

The statement of associativity is the following:

{\begin{verbatim}
Forall(A:type)(n1 n2 n3 : nat)
      (l1 : <vec A n1>)
      (l2 : <vec A n2>)
      (l3 : <vec A n3>).
  { (append (append l1 l2) l3) =
    (append l1 (append l2 l3)) }
\end{verbatim}}

\noindent Since for \texttt{append}, the lengths
\texttt{n1}, \texttt{n2}, and \texttt{n3} are specificational data,
they are dropped in type-free positions.  Hence, the equation to be
proved does not mention those lengths.  In type theories like Coq's or
Epigram's, in contrast, the equation to be proved must be typed, and
so must mention the lengths:

{\begin{verbatim}
{ (append (plus n1 n2) n3
     (append n1 n2 l1 l2) l3) =
  (append n1 (plus n2 n3)
       l1 (append ! n2 n3 l2 l3)) }
\end{verbatim}}

\noindent In fact, since the two sides of this latter equation have,
respectively, types \texttt{<vec A (plus (plus n1 n2) n3)>} and
\texttt{<vec A (plus n1 (plus n2 n3))>}, even stating this theorem
requires heterogeneous equality.  The proof of the equality must
contain in it a proof of associativity of addition.  In contrast,
in \optt, since the lengths are dropped, the proof of associativity
is just as for \texttt{append} on lists without length.  The proof
does not require associativity of \texttt{plus}.  

\subsection{Lists with Iterated Difference}
\label{sec:eg2}

In the previous example, we prove terms equal which have provably
equal but not definitionally equal types.  Such proofs can be done in
type theories like Coq's.  We may push this example further in \optt,
however, to prove terms equal with types which are not even provably
equal.  Consider the following list type, where the type accumulates
not the length of the list, but the iterated difference of the list
values:

\begin{verbatim}
Inductive mvec : Fun(n:nat).type :=
  mvecn : <mvec Z>
| mvecc : Fun(spec n:nat)(x:nat)(l:<mvec n>).
            <mvec (minus x n)>.
\end{verbatim}

\noindent We may define \texttt{append} on such lists as before
(although here we do not bother to specify the relationship between
the index of the output list and that of the input lists).

\begin{verbatim}
Inductive append_t : type :=
  mk_append_t: Fun(spec n:nat)(l:<mvec n>).append_t.

fun append(spec n m:nat)
          (l1 : <mvec n>)(l2 : <mvec m>):
          append_t.
  ...
\end{verbatim}

\noindent We may again prove associativity of \texttt{append}:

\begin{verbatim}
 Forall(n1 : nat)(l1 : <mvec n1>)
       (n2 n3 : nat)(l2 : <mvec n2>)(l3 : <mvec n3>)
       (n12 n23:nat)(l12:<mvec n12>)(l23:<mvec n23>)
       (u1:{ (append l1 l2) = (mk_append_t l12)})
       (u2:{ (append l2 l3) = (mk_append_t l23)}).
   { (append l12 l3) = (append l1 l23) } 
\end{verbatim}

\noindent The critical difference here is that, due to the
non-associativity of iterated difference, the type indices in question
are not provably equal.  In fact, we may easily construct examples
where they are provably disequal.  For example, take the singleton
lists containing 3, 2, and 1, respectively: the two orders of
appending give rise to indices 0 and 2.  \optt\ allows provably equal
terms with disequal types, because equality in \optt\ is operational
equality on type-free terms.  Provable equality in type theories like
Coq's are not operational in this sense, and theorems like
associativity of append on lists with iterated difference cannot be
proved.

\subsection{Untyped Lambda Calculus Interpreter} 

We internally verify that a simple call-by-value interpreter for the
untyped lambda calculus maps closed terms to closed terms.  For
substitution on open terms, see the case study below
(Section~\ref{sec:golfsock}).  The datatype for lambda terms $t$ is
indexed by the list of $t$'s free variables.  Using explicit names for
free variables is adequate for our purposes here.  We take names to be
natural numbers.  The datatype of terms is:

\begin{verbatim}
Inductive lterm : Fun(l:<list nat>).type :=
  var : Fun(v:nat).
           <lterm (cons nat v (nil nat))>
| abs : Fun(a:nat)(l:<list nat>)(b:<lterm l>).
           <lterm (removeAll nat eqnat a l)>
| app : Fun(l1 l2:<list nat>)
           (x1:<lterm l1>)(x2:<lterm l2>).
           <lterm (append nat l1 l2)>.
\end{verbatim}

\noindent Here, \texttt{removeAll} removes all occurrences of an
element from a list, and \texttt{append} appends lists (without
length).  The crucial helper function is for substitution of a closed
term \texttt{e2} for a variable \texttt{n} into an open term
\texttt{e1}, with list of free variables \texttt{l}.  This
substitution function has the following type:

\begin{verbatim}
Fun(e2:<lterm (nil nat)>)(n:nat)
   (l:<list nat>)(e1:<lterm l>).
   <lterm (removeAll nat eqnat n l)>
\end{verbatim}

\noindent Note that here we are internally verifying a certain
relationship between the sets of free variables of the input terms and
the output term.  Internally, this code uses several (external) lemmas
about \texttt{removeAll}.  Where substitution enters another lambda
abstraction, a commutativity property is required, saying that
removing $x$ and then removing $y$ results in the same list as
removing $y$ and then $x$.  Using substitution, we can implement
$\beta$-reduction for closed redexes in the interpreter.  The
interpreter then has the following type, which verifies internally
that evaluation of a closed term, if it terminates, produces a closed
term:

\begin{verbatim}
Fun(e1:<lterm (nil nat)>).<lterm (nil nat)> 
\end{verbatim}

We also externally verify that if this interpreter terminates, then
its result is a lambda abstraction.  Here, we do not bother to track
the fact that the list of free variables in the resulting abstraction
is empty (this could be easily done).

\begin{verbatim}
Forall(e1 e2:<lterm (nil nat)>)
      (p:{(lterm_eval e1) = e2}).
  Exists(a:nat)(l:<list nat>)(b:<lterm l>).
    { e2 = (abs a b) }
\end{verbatim}

\noindent The proof of this property relies on a principle of
computational induction, stated in Section~\ref{sec:eqext} below, for
reasoning by induction on the structure of a computation which is
(assumed here to be) terminating, namely \texttt{(lterm\_eval e1)}.

\section{Termination Casts}
\label{sec:terminates}

Universal elimination and existential introduction require terminating
terms, up until now taken to be just constructor terms.  Suppose we
want to instantiate a universal using a non-constructor term $(f\
\bar{a})$, where for simplicity suppose $\bar{a}$ are constructor
terms.  Using the proof rules given above, one would first prove
totality of $f$: for all inputs $\bar{x}$, there exists an output $r$
such that $(f\ \bar{x}) = r$.  Instantiating $\bar{x}$ with $\bar{a}$
and then performing existential elimination will provide a variable
$r$ together with a proof $u$ that $(f\ \bar{x}) = r$.  Now the
original universal instantiation can be done with $r$, translating
between $r$ and $(f\ \bar{x})$ as necessary using equational reasoning
and $u$.  If this strikes the reader as somewhat tedious, that is
indeed the authors' experience.  Matters are even worse with nested
non-constructor applications, where the process must be repeated in a
nested fashion.

To improve upon this, we extend \texttt{Terminates} from constructor
terms to provably total terms, as follows.  We introduce a new term
construct of the form $\texttt{terminates}\ t\ \texttt{by}\ P$.  This
is a termination cast.  Where a type cast changes the type checker's
view of the type of a term, a termination cast changes its view of the
termination behavior of a term.  \texttt{Terminates} is extended to
check that $P$ either proves $t$ is a equal to a constructor term, or
else proves totality of the head (call it $f$) of $t$, in the sense
mentioned above.  The basic design of \optt\ makes this addition
straightforward, since termination casts are computationally
irrelevant.  We extend our definitional equalities by dropping
\texttt{terminates}-annotations:
\[
\begin{array}{lll}
$\texttt{terminates}\ t\ \texttt{by}\ P$ & \To & t
\end{array}
\]
\noindent Termination casts may be used in universal instantiation and
existential introduction, but are eliminated by definitional equality
during equational reasoning.

\section{Functional Modeling, Ownership Annotations}
\label{sec:model}

Inspired by a suggestion of Swierstra and Altenkirch, we sketch an
extension of \optt\ to support non-functional operations like
destructive updates and input/output via functional
modeling~\cite{swierstra+07}.  The basic idea is to define a
functional model of the non-functional operations.  This model can be
used for formal reasoning.  It is replaced during compilation by its
non-functional implementation, which must be trusted correctly to
implement the functional model.  To ensure soundness, usage of the
functional model in code is linearly restricted.  Swierstra and
Altenkirch propose using monads for this.  Here, we use uniqueness
types~\cite{barendsen+93}.  Types and type families can be designated
as \emph{opaque}, in which case any functions which perform case
analysis on them must be marked specificational.  Functions marked
specificational must be replaced during compilation.

We extend \optt\ with ownership annotations \texttt{unique} and
\texttt{unique\_owned}, qualifying function inputs similarly to
\texttt{spec}.  Inputs marked \texttt{unique} must be consumed by the
function exactly once.  Those marked \texttt{unique} may be used but
must not be consumed.  Term constructors may take \texttt{unique} (but
not \texttt{unique\_owned}) arguments.  Applications of such to
\texttt{unique} expressions become \texttt{unique} as well, consuming
the resource.  Functions marked specificational need not obey
uniqueness requirements, since they will be replaced by trusted
non-functional implementations.  A simple static analysis ensures
correct resource usage.

\optt's distinction between computational and specification
definitional equality is here crucial.  In specificational functions,
we use the specificational equality, which takes definitions of opaque
types into account.  In computational functions, we use the
computational equality, which does not.  So computational functions
may not violate the abstraction boundary imposed opacity.  For
example, if 32-bit words are modeled as vectors of booleans of length
32, then operations on vectors must not in general be applied to
words; only those marked as specificational, which will be replaced
during compilation.  Ownership annotations are dropped in the
specificational equality, reducing clutter during external reasoning.
Examples of this approach are described below
(Section~\ref{sec:golfsock}).

\section{Reference Counting and Compilation}
\label{sec:compilation}

Since all data in \optt\ are inductive, the data reference graph is
truly acyclic.  So \textsc{Guru}'s compiler (to C code) implements
memory reclamation using reference counting, instead of garbage
collection.  Reference counting is sometimes criticized as imposing
too much overhead, due to frequent increments and decrements.
\textsc{Guru} puts this under the control of the programmer via
explicit \texttt{inc}s and \texttt{dec}s.  But \textsc{Guru} also
provides ownership annotations to reduce the need for these.  Function
inputs may be marked as \texttt{owned} by the calling context.  To
consume them, the function must do an \texttt{inc}.  But just to
inspect them by pattern matching does not require an \texttt{inc}, and
the function may not\texttt{dec} an \texttt{owned} input.  The static
analysis mentioned above for tracking uniqueness also ensures correct
reference counting.  Pattern matching consumes unowned resources.
Functions and flat inductive data like booleans are not tracked.  The
former is sound here because \textsc{Guru} does not implement closure
conversion.  Closures may be implemented by hand, thanks in part to
\optt's System-F-style polymorphism.  Reclaimed data items are placed
on per-constructor free lists.  When one of these is used to satisfy a
new allocation request, its subdata are put on their free lists.  We
thus get an incremental memory reclamation with much more constrained
worst-case running time than garbage collection.  \optt\ again helps
here, since we make increments and decrements (of terminating terms)
computationally irrelevant via definitional equality.  They need not
be considered during formal reasoning.

\section{Cut Elimination}
\label{sec:cut_elim}

In this section we sketch consistency of the logic via analysis of
canonical forms of closed proofs of atomic formulas, obtained by a
two-step normalization process.  First, standard logical cuts are
removed, then equational cuts.

\subsection{Logical Cut Elimination}
\label{sec:lce}

Because \optt's logic has been designed to have a low proof-theoretic
strength, a textbook proof of strong normalization for reduction of
logical cuts can be applied with only a few minor adaptations.  

\begin{theorem}[Logical Cut Elimination]
Logical cuts can be removed by reduction in a finite number of steps
from any \optt\ proof.
\end{theorem}

Next, we must establish type safety for reductions of logical cuts.
Type preservation is easily established, thanks to proof irrelevance.

\begin{theorem}[Type Preservation for Logical Reduction]
If $\seq{\Gamma}{P}{F}$ and $P\ \leadsto\ P'$, then $\seq{\Gamma}{P'}{F}$.
\end{theorem}

%\noindent
%As usual, most of the work in proving this is done in a substitution
%lemma.  Only substitution of an inactive expression need be
%considered, due to the value restriction on instantiation of universal
%quantifiers.
%
%\begin{lemma}[Substitution for Proofs]
%If $\seq{\Gamma_1,x:A,\Gamma_2}{P}{F}$ and $\seq{\Gamma_1}{I}{A}$,
%then $\seq{\Gamma_1,[I/x]\Gamma_2}{[I/x]P}{[I/x]F}$.
%\end{lemma}
%
%\noindent Note that the set of inactive terms is closed under
%substitution by inactive terms, so this substitution cannot given rise
%to a syntactically ill-formed (pseudo-)proof.  

The second part of type safety is progress.  A complication arises
here, due to the possible presence of casts in constructor terms.  We
assume an obvious notion of being stuck.

\begin{theorem}
If $\seq{\,}{P}{F}$, then $P$ is not stuck.
\end{theorem}

\textbf{Cast Shifting.} We must show that $P$ cannot get stuck because
an \texttt{induction}-proof analyzing datatype $\langle d\ \bar{Y}
\rangle$ is applied to a term containing some casts.  Briefly, this is
done by showing that we may always shift such casts out of the way,
using an approach similar to that used by Chapman in a formalization
of a Martin-L\"of type theory~\cite{chapman08}.  Casts between types
of different structure (such as a function of 3 arguments to a
function of 2) must contain a proof of an equation that contradicts
our equational theory (where such types are provably disequal).  The
entire subproof can then be eliminated by the consequent contradiction.

\subsection{Equational Cut Elimination}
\label{sec:eqcuts}

Every equation $\{t\, \Eq\, t'\}$ or $\{T\,\Eq\, T'\}$ provable in the
empty context satisfies one of the following properties (modulo
definitional equality):

\noindent
\textbf{1.} $t$ and $t'$ are both diverging.

\noindent
\textbf{2.} $t$ and $t'$ are joinable at an inactive or stuck term,
the latter considered because we may reason about ill-typed terms.

\noindent 
\textbf{3.} $T$ and $T'$ are of the forms $R[\bar{t}]$ and $R[\bar{t}']$ for
some type expression $R$ with holes in any non-binding position, and
lists of terms $\bar{t}$ and $\bar{t'}$, with corresponding elements
provably equal.

\noindent A related characterization for disequations is omitted here.

\begin{theorem}[Consistency]
There are atomic formulas which are not provable in the empty context.
\end{theorem}

\begin{theorem}
\label{thm:types}
Types provably equal in the empty context have the same type
structure.
\end{theorem}

%\begin{theorem}[Intensionality]
%Equality between terms in \optt\ is intensional.
%\end{theorem}

%We may observe also that except for necessary incompleteness in
%proving diverging terms are equal, \optt's equational inferences can
%derive all true equations and disequations in the empty context of the
%kind characterized, without using \texttt{inj}, \texttt{injf}, or
%\texttt{contra}.  We can thus provably identify those rules as
%essentially for hypothetical reasoning, and also claim:

%\begin{theorem}[Equational Completeness]
%All equations and disequations of the forms characterized above which
%do not rely on equating diverging terms and which are true in the
%empty context are provable in the empty context in \optt.
%\end{theorem}

\section{Type Soundness}
\label{sec:typesound}

We state type soundness using a typeful version of the operational
semantics, which operates on terms with all their type and proof
annotations.  This typeful evaluation must insert casts as it goes to
preserve typing.  Consider evaluation of a term $E[R]$, with $E$ an
evaluation context, which happens to be of type $T[R]$, with $R$ a
redex.  Suppose we contract $R$ to $t$, causing $E[R]$ to reduce to
$t'$.  Then the type becomes $T[t]$.  If the hole in $T$ is
immediately inside a type inside a term, there is no difficulty:
$T[t]$ is definitionally equal to $T[R]$, since types inside terms are
dropped by definitional equality.  If the hole in $T$ does not occur
inside a type inside a term, then $E[t]$ might not be definitionally
equal to $E[R]$.  So typeful evaluation must insert a cast around
$t'$, using a proof that $\{ T[t]\,\Eq\,T[R]\}$.  Type-preserving
evaluation must also shift casts off of applied functional terms and
off terms scrutinized by \texttt{match}.  The shifting technique
described above works, unless the cast uses a proof of an equation
violating type structure.  But this cannot happen in the empty
context, thanks to Theorem~\ref{thm:types}.

\begin{theorem}[Preservation]
If $t:T$ and $|t|\leadsto t'$ 
then there exists $t''$ with $|t''| = t'$ such that $t'' : T$.
\end{theorem}

\begin{theorem}[Progress]
If $t:T$, then $t$ is not stuck.
\end{theorem}

\section{Equational Extensions}
\label{sec:eqext}

In \optt, as in \textsc{OTT}, extension of the equational theory is
much easier than in traditional intensional type
theories~\cite{altenkirch+07}.  Here, we see how to extend \optt\ with
new equational reasoning principles.  For each extension, we must
verify that it cannot violate the property stated for (unextended)
\optt\ in Theorem~\ref{thm:types}, since type soundness depends upon
this.  

%Since all new equations and disequations are between terms,
%that property can be violated only if the theory becomes inconsistent.

\textbf{Finite Failure Clash.} The following simple principle can be
easily added:
\[
\infer{
\seq{\Gamma}{\texttt{aclash}\ I}{\{\texttt{abort\,!}\ \Neq\ I\}}
 }
{\ } 
\]

\textbf{Parametric Extensionality.} We may extend the theory with a
limited form of extensionality for \texttt{fun}-terms using the
following rule:
\[
\infer{
\seq{\Gamma}{\texttt{pext}\,r(\bar{x}:\bar{A}). P}{\{\texttt{fun}\, r(\bar{x}:\bar{A}).t\, \Qeq\, \texttt{fun}\,r(\bar{x}:\bar{A}).t'\}}}
{\seq{\Gamma}{P}{\{t\ \Qeq\ t'\}}}
\]
This principle allows us to equate abstractions by equating their
bodies, but only using equational inferences, and critically, not
induction.  This is because the context is not extended, so only
(untyped) equational reasoning is possible.  

With full extensionality, the context is extended.  This principle is
not compatible with \optt, for with it, we could prove that
$\texttt{fun}(x:\textit{empty}).0$ is equal to
$\texttt{fun}(x:\textit{empty}).1$.  This is because we would have an
assumption that $\textit{empty}$ is inhabited.  But the equational
theory of \optt\ does not require type correctness of applications.
Hence, these \texttt{fun}-terms could be applied to terms (which do
not have type \textit{empty} in the empty context), and then reduced
using \texttt{join}.  We would easily derive $\{0\,\Eq\,1\}$ this way.
A more typeful theory of equality could solve this problem, but
carries its own costs.

\textbf{Computational Inversion.} We may deduce that a term $t$
terminates from a proof that $E[t]$ terminates (where $E$ is an
evaluation context).  In the empty context, this principle is valid
under our characterization of provable equality.

\[
\infer{
\seq{\Gamma}{\texttt{cinv}\ t\ P}{\texttt{Exists}(x:T).\{t\ \Eq\ x\}} }
{\seq{\Gamma}{P}{\{E[t]\ \Eq\ I\}} & \seq{\Gamma}{t}{T}}
\]

\textbf{Computational Induction.}  Stronger than computational
inversion is a principle of computational induction, indirectly
supported as follows.  Suppose defined a standard inductive datatype
for unary natural numbers.  For any term $t$ of the form
$\texttt{fun}\ r(\bar{x}:\bar{A}):T.t'$, let $t^\textit{nat}$ be the
following term:
\[
\begin{array}{l}
\texttt{fun}\ r(n:\textit{nat})(\bar{x}:\bar{A}):T.\\
\ \ \ \ \ \texttt{match}\ n\ \texttt{by}\ x\ y\ \texttt{with}\\
\ \ \ \ \ \ \ \ \ \texttt{Z}\ \texttt{=>}\ \texttt{abort !} \\
\ \ \ \ \ \ \ | \ \texttt{S\ n'}\ \texttt{=>}\ [(r\ n')/r]t' \\
\ \ \ \ \ \texttt{end}
\end{array}
\]
This is just like $t$, except that it takes a \textit{nat} $n$ to use
as computational budget in a standard way: when the budget is out,
$t^\textit{nat}$ aborts; otherwise it behaves like $t$.

Suppose for simplicity that all functions used in the body $t'$, other
than $r$, are provably terminating.  Then termination (without
aborting) of $t^\textit{nat}$ on some inputs provably entails
termination of $t$ on those inputs, in unextended \optt.  The
converse, however, does not appear to be provable.  Provable
equivalence of termination for $t$ and $t^\textit{nat}$ would allow
reasoning by induction on the structure of a terminating computation
of $t$ indirectly, by reasoning by induction on the computational
budget $n$ for a terminating computation of $(t^\textit{nat}\ n)$.  To
obtain provable equivalence, we may extend \optt\ with the following
principle, which is already valid in the empty context under our
characterization of provable equality:
\[
\infer{
\seq{\Gamma}{\texttt{cind}\ P}{\texttt{Exists}(n:\textit{nat}).\{(t^\textit{nat}\ n\ \bar{X})\ \Eq\ I\}} }
{\seq{\Gamma}{P}{\{(t\ \bar{X})\ \Eq\ I\}}}
\]

\section{Case Study: Incremental LF}
\label{sec:golfsock}

This section describes a larger case study carried out in \optt, in
the domain of efficient proof checking.  In automated theorem proving,
the complexity of solver implementations limits trustworthiness of
their results.  For example, modern SMT solvers typically have
codebases around 50k-100kloc C++ (e.g., \textsc{CVC3}~\cite{BT07}).
One method to help catch solver errors and to export results to
skeptical interactive theorem provers is to have the solvers emit
proofs.  Independent checking of the proofs by a much smaller and
simpler checker can confirm the solver's results.  Efficienct and
flexible proof checking for tools like SMT solvers is a subject of
current interest in the SMT community (e.g.,~\cite{moskal08}).  A
proposal of the first author is to use an extension of the Edinburgh
Logical Framework (LF) as the basis for efficient and flexible proof
checking for SMT~\cite{SO08,HHP93}.  LF is a dependent type theory
with support for higher-order abstract syntax, used previously in
proof-carrying code and related applications (e.g.,~\cite{A01,N97}).
In LF encoding methodology, proof checking in an object logic is
reduced to type checking in LF.  To handle large proofs from SMT
solvers, several optimizations for LF type checking have been
proposed, including \emph{incremental checking}.

\subsection{Incremental LF Type Checking}

Incremental checking intertwines parsing and type checking for
LF~\cite{stump08b}.  The goal is to avoid creating abstract syntax
trees (ASTs) in memory whenever possible.  ASTs must be created for
expressions which will ultimately appear in the type of a term, but
others need not.  This gives rise to one pair of modes, namely
creating vs. non-creating.  Standard bi-directional type checking for
canonical forms LF gives rise to an orthogonal pair of modes, namely
type synthesizing (computing a type for a term in a typing context)
vs. type checking (checking that a term has a given type in a typing
context)~\cite{clf02,pierce98local}.  To check a term, we are
initially in non-creating mode.  When we encounter an application with
head term of type $\Pi x : A.\, B$, where $x$ is free in $B$, we must
switch to creating mode to check the argument term.  If $x$ is not
free in $B$, we may remain in non-creating mode, thus avoiding
building an AST for the argument.  An implementation of incremental
checking in around 2600 lines of C++ has been evaluated on benchmark
proofs generated from a simple quantified boolean formula (QBF)
solver~\cite{stump08b}.  The results show running times faster than
those previously achieved by \emph{signature compilation}, where a
signature is compiled to an LF checker customized for checking proofs
in that signature~\cite{zeller07}.  Implementing the incremental type
checker in C++ is quite error-prone, due to lack of memory safety in
C++, and the dependence of outputs on requested checking modes.

\subsection{Incremental Checking in \textsc{Guru}}

An incremental LF checker called \textsc{Golfsock} has been
implemented in \textsc{Guru}, where we internally verify two
properties.  First, mode usage is consistent, in the sense that if the
core checking routine is called with a certain combination of modes
(from the orthogonal pairs checking/synthesizing and
creating/non-creating), then the appropriate output will be produced:
the term, iff in creating mode; and its type, iff in synthesizing
mode.  Second, whenever a term is created, there is a corresponding
typing derivation for it in a declarative presentation of LF.
\textsc{Golfsock} is somewhat usual compared with related examples
(e.g.,~\cite{urban+08}), due to the need to use more efficient data
structures than typically used in mechanized metatheory.

\subsection{Machine Words for Variable Names}

\textsc{Golfsock} uses 32-bit machine words for variable names.  An
earlier version used unary natural numbers for variables, but
performance was poor, with profiling revealing 97\% of running time on
a representative benchmark going to testing these for equality.
Replacing unary natural numbers with 32-bit words resulted in a 60x
speedup on that benchmark.  But using 32-bit words for variable names
requires significant additional reasoning in \textsc{Golfsock}.  The
reason is that capture-avoiding substitution relies on having a strict
upper bound for the variables (bound or free) involved in the
substitution.  This strict upper bound is used to put the term into
$\alpha$-canonical form during substitution: all bound variables
encountered are renamed to values at or above the upper bound, thus
ensuring that free variables are not captured.  We must maintain the
invariant that new upper bounds produced by functions are always
greater than or equal to the initial upper bounds.  This requires
incrementing of 32-bit words and inequality, as well as associated
lemmas.  Fortunately, the \textsc{Guru} standard library includes an
implementation of bitvectors (as vectors of booleans), with an
increment function, functions mapping to and from unary natural
numbers, and appropriate lemmas about these.  These are specialized to
vectors of length 32 for machine words.  For \textsc{Golfsock}, the
most critical of these are \texttt{word\_inc}, which increment a
32-bit word, reporting if overflow occurred; and the following lemma
stating that if incrementing word \texttt{w} produces \texttt{w2}
without overflow (\texttt{ff} is boolean false), then mapping
\texttt{w2} to a unary natural number gives the successor of the 
result of mapping \texttt{w}.

\begin{verbatim}
Define word_to_nat_inc2
   : Forall(w w2:word)
           (u : { (word_inc w) = 
                  (mk_word_inc_t w2 ff)}).
      { (S (word_to_nat w)) = (word_to_nat w2) }
\end{verbatim}

\noindent To use this lemma, \textsc{Golfsock} aborts if overflow
occurs.  Provisions are included to reset the upper bound in
non-creating, checking mode, where this is proven sound; and overflow
does not occur in any benchmarks tested.  A more robust solution, of
course, is to use arbitrary precision binary numbers.  Implementation
of these is in progress but currently not available.

Following the methodology described in Section~\ref{sec:model}, the
\texttt{word} datatype is treated as opaque, with the critical
computational operations on words replaced during compilation.  The
fact that these replacements are functionally equivalent to the
operations as modeled in \textsc{Guru} is unproven and must be
trusted.  Fortunately, there are just three such operations used in
\textsc{Golfsock}: creating the word representing 0, incrementing a
word with overflow testing, and testing words for equality.  These
total just 8 lines of C code.

\subsection{Tries and Character-Indexed Arrays}

A trie is used for efficiently mapping strings for globally or locally
declared identifiers to variables (32-bit words) and the corresponding
LF types.  Tries are implemented in the standard library with the
following declaration:

\begin{verbatim}
Inductive trie : Fun(A:type).type :=
  trie_none : Fun(A:type).<trie A>
| trie_exact : Fun(A:type)(s:string)(a:A).<trie A>
| trie_next : Fun(A:type)(o:<option A>)
                 (unique l:<charvec <trie A>>). 
              <trie A>.
\end{verbatim}

\noindent The first constructor is for an empty trie, the second for a
trie mapping just one string to a value, and the third for a trie
mapping multiple strings to values.  The second and third overlap in
usage: we can map a single string to a value using one
\texttt{trie\_exact} or a nesting of \texttt{trie\_next}s.  This
\texttt{trie\_next} uses an opaque datatype \texttt{charvec} for
character-indexed arrays, where characters are 7-bit words (for ASCII
text only).  These arrays are modeled functionally as vectors of
length 128.  We statically ensure that array accesses within bounds,
since the vector read function requires a proof of this.  Destructive
array update is supported with uniqueness types, ensuring access
patterns consistent with destructive modification.  During
compilation, the functional model is replaced by an implementation
with actual C arrays, and constant-time read and write operations.
Operations implemented on tries include insertion, lookup, and
removal, as well as a function \texttt{trie\_interp} which maps a trie
to a list of (key,value) pairs.

The fact that \texttt{trie\_next} contains a character-indexed array
of tries poses a challenge for proving theorems about tries.  The
problem is that trie operations access subtries of a trie \texttt{T}
via an array read.  In the functional model, the resulting subtrie is
not a structural subterm of \texttt{T}, and so proof by induction on
trie structure cannot apply an induction hypothesis to the subtrie.
This problem may not seem difficult: informal reasoning can easily get
around this problem using instead complete induction on the size of
the trie.  But how can we write a provably total function to compute
the size of a trie?  Such can certainly be implemented in
\textsc{Guru}, but to prove it total we are back to the same problem
it was introduced to solve: the natural totality proof proceeds
essentially by induction on the structure of the trie.  The separation
of terms and proofs in \optt\ provides a foundation for an easy fix to
this problem.  We introduce a specificational construct \texttt{size
t} to compute the size of any value.  Functions are assigned size 0,
while constructor terms are assigned the successor of the sizes of
their subterms.  The evaluation rules of the theory are extended
appropriately.  We may now prove properties about trie operations by
complete induction on trie size computed by this construct.  \optt's
design allows us to avoid distracting problems such as specifying a
sensible behavior of the \texttt{size} construct on proofs, types, and
formulas.

As a performance benchmark, a program to histogram the words in ASCII
text was implemented in both \textsc{Guru} and \textsc{OCaml} version
3.10.1.  Runtimes are indistinguishable with array-bounds checking on
or off in the \textsc{OCaml} version.  Note that array accesses are
statically guaranteed to be within bounds in the \textsc{Guru}
version.  The same data structures, particularly mutable tries, and
algorithms were implemented in each.  Counting the number of times the
word ``cow'' occurs in an English translation of ``War and Peace'' (it
occurs 3 times) takes 3.7 seconds with the \textsc{OCaml} version on a
standard test machine, and 1.5 seconds with the \textsc{Guru} version.
Disabling garbage collection in \textsc{OCaml} drops the runtime to
1.2 seconds.  While hardly conclusive, this experiment supports the
hypothesis that programmer-controlled reference counting may not be
inferior to garbage collection, at least for some applications.  This
is consistent with the results of a thorough study showing that
garbage collection may be significantly slower than more fine-grained
memory management schemes in memory-constrained
settings~\cite{hertz+05}.

\subsection{Specifications and Proofs}

The central routine, called \texttt{check}, to perform LF type
checking on textual input has the following type, which we will
consider in detail.  The notation \texttt{@<...>} is for formula-level
application of defined predicates (not mentioned previously), easily
accomodated in definitional equality.

\begin{verbatim}
Fun(unique pb_stdin:pb_stdin_t)
   (unique symbols:symbols_t)
   (nextid:var)
   (spec symok:@<symbols_ok nextid symbols>)
   (create:bool)
   (expected:<option trm>)
   (bndexpected:{(bndopttrm nextid expected) = tt})
   (owned where:string).
  unique <check_t nextid symbols create expected>
\end{verbatim}

\noindent This type says that \texttt{check} consumes a
\texttt{pb\_stdin} (``pushback standard input''), which supports
reading the next character from stdin and pushing characters back; a
symbol table; the next identifier to use (\texttt{var} is defined to
be \texttt{word}, for 32-bit words, as discussed above); a proof of an
invariant relating the symbol table and \texttt{nextid}, the latter as
an upper bound on symbols in the symbol table; a flag \texttt{create}
controlling whether we are in creating or non-creating mode; an
optional \texttt{expected} expression, which is supplied if we are in
checking mode and omitted in synthesizing mode; a proof that if the
optional expression is present, all variables in it are strictly
bounded above by \texttt{nextid}; and a string to be printed in any
error messages generated.  Given these inputs, \texttt{check} produces
a unique element of the type \texttt{<check\_t nextid symbols create
expected>}, defined with a single constructor as follows:

\begin{verbatim}
Inductive check_t
  : Fun(nextid:var)(symbols:symbols_t)
       (create:bool)(expected:<option trm>).type :=
  mk_check 
  : Fun(spec nextid:var)(spec symbols:symbols_t)
       (spec create:bool)(spec expected:<option trm>)
       (unique pb_stdin:pb_stdin_t)
       (unique symbols':symbols_t)
       (nextid':var)
       (spec T:trm)
       (k:<tcheck_t nextid' symbols create T>)
       (K:<Tcheck_t nextid' expected T>)
       (nle : { (vle nextid nextid') = tt })
       (U : { (trie_interp symbols) = 
              (trie_interp symbols')}).
     <check_t nextid symbols create expected>.
\end{verbatim}

\noindent This \texttt{mk\_check} constructor first takes four
specificational arguments needed to describe relationships between the
computational outputs of \texttt{check} and its inputs.  It then
packages up these outputs: the next \texttt{pb\_stdin} to use (since
the one input to \texttt{check} is consumed by reading from it); an
updated symbol table \texttt{symbols'}; the next identifier to use
(\texttt{nextid'}); two data elements \texttt{k} and \texttt{K}, to be
discussed next, related via a specificational type \texttt{T}; a proof
(\texttt{nle}) that the original \texttt{nextid} is less than or equal
to the new \texttt{nextid'}; and a proof that the updated symbol table
has the same interpretation, as a list of (key,value) pairs, as the
starting one.  Note that the symbol tables in general will not be
equal, even though their interpretations are, since inserting and then
removing an element into the table may result in a table with a
different form (due to the overlap between \texttt{trie\_exact} and
\texttt{trie\_next}, mentioned above).

The data element \texttt{k} packages up a term and a specificational
declarative LF typing derivation for it if we are in creating mode;
and otherwise packages up nothing computational.  The appropriate
value for the \texttt{create} flag is used as an index in each case,
to enable static checking of consistent mode usage:

\begin{verbatim}
Inductive tcheck_t
  : Fun(nextid:var)(symbols:symbols_t)
       (create:bool)(T:trm).type :=
  tcheck_ff
  : Fun(spec nextid:var)
       (spec symbols:symbols_t)
       (spec T:trm).
      <tcheck_t nextid symbols ff T>
| tcheck_tt
  : Fun(spec nextid:var)
       (spec symbols:symbols_t)
       (spec T:trm)
       (t:trm)
       (spec d:<deriv (gs_ctxt symbols) t T>)
       (bt : { (bndtrm nextid t) = tt}).
     <tcheck_t nextid symbols tt T>.
\end{verbatim}

\noindent The type \texttt{<deriv (gs\_ctxt symbols) t T>} in the
second case is inhabited by encoded proofs in a declarative
presentation of LF, to show that \texttt{t} has type \texttt{T} in
context \texttt{(gs\_ctxt symbols)}.  This function \texttt{gs\_ctxt}
is based on the \texttt{trie\_interp} function mentioned above: it
maps a symbol table to a typing context, discarding the strings in the
symbol table.

The final piece of the output of \texttt{check}, the item \texttt{K},
packages up either a synthesized type (if in synthesizing mode), or
else a proof of $\alpha$-equivalence of the expected type and the type
\texttt{T} which the created term (if any) has.  The appropriate
member of the \texttt{option} type indexes \texttt{Tcheck\_t}, again
to enable statically verifying consistent mode usage.

\begin{verbatim}
Inductive Tcheck_t
 : Fun(nextid:var)
      (expected:<option trm>)
      (T:trm).type :=
  Tcheck_nothing
  : Fun(spec nextid:var)
       (T:trm)
       (bT:{(bndtrm nextid T) = tt}).
     <Tcheck_t nextid (nothing trm) T>
| Tcheck_something
  : Fun(spec nextid:var)
       (spec T eT:trm)
       (spec nextid':var)
       (nle:{(vle nextid nextid') = tt})
       (u : { (acanon nextid' T) =
              (acanon nextid' eT)}).
     <Tcheck_t nextid (something trm eT) T>.
\end{verbatim}

\noindent The function \texttt{acanon} is for putting terms in
$\alpha$-canonical form.  Allowing $\alpha$-equivalence from a
\texttt{nextid'} greater than or equal to the current \texttt{nextid}
facilitates resetting the next id when we are in non-creating/checking
mode.

\subsection{Statistics}

The code for the \texttt{check} routine is around 1100 lines.  Its
size would make it challenging to reason about externally, so
verifying it internally with dependent types seems the right choice.
\textsc{Golfsock} proper is around 4000 lines of code and proofs,
resting upon files from the \textsc{Guru} standard library totally an
additional 6700 lines, mostly of proofs.  The \textsc{Guru} compiler
produces 9000 lines of C for \textsc{Golfsock}.  A number of lemmas
remain to be proved.  Even so, they are more trustworthy then the
several thousand lines of complex C++ code of the first author's
original unverified incremental checker. This increase in
trustworthiness can be confirmed anecdotally.  The first author
encountered just a couple of relatively benign bugs while developing
it (related to properties not selected to be verified), in contrast to
a long and laborious debugging effort needed for the original
unverified implementation.

\subsection{Empirical Results}

Figure~\ref{fig:qbf} gives empirical results comparing the original
C++ implementation (``C++ impl'') with \textsc{Golfsock}, and also
\textsc{Twelf}~\cite{PfS98}.  The primary usage of \textsc{Twelf} is
for machine-checked meta-theory (e.g.,~\cite{lee+07}), not checking
large proof objects.  \textsc{Twelf} is included here as a well-known
LF checker not written or co-written by the first author.  The
benchmarks used are the QBF ones mentioned above, originally
considered in the work on signature compilation~\cite{zeller07}.
Note that while the C++ checker has support for a form of term
reconstruction (also known as implicit arguments), \textsc{Golfsock}
does not, and hence we use the fully explicit form of these
benchmarks.  The results show \textsc{Golfsock} is around 50\% slower
than the C++ version.  We may consider this a good initial result,
particularly since the C++ version implements many optimizations not
supported in \textsc{Golfsock}.  For example, the C++ version
implements a form of delayed substitution, while \textsc{Golfsock}
substitutes eagerly.  Each such optimization which the C++
implementation can include at no (initial) cost would need to be
verified in the \textsc{Golfsock} version, with respect to declarative
LF typing.

\begin{figure}
\footnotesize
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{benchmark} & \textbf{size (MB)} & C++ impl & \textsc{Golfsock} & \textsc{Twelf}
\\
\hline
cnt01e
&
2.6
&
1.3
&
2.0
&
14.0 
\\
tree-exa2-10
&
3.1
&
1.7
&
2.5
&
18.6
\\
cnt01re
&
4.6
&
2.4
&
3.6
&
218.4
\\
toilet\_02\_01.2
&
11
&
5.8
&
8.8
&
1143.8
\\
1qbf-160cl.0
&
20
&
10.0
&
14.1
&
timeout
\\
tree-exa2-15
&
37
&
19.9
&
31.2
&
timeout 
\\
toilet\_02\_01.3
&
110
&
58.6
&
89.7
&
exception 
\\
\hline
\end{tabular}
\end{center}
\caption{\label{fig:qbf}Checking times in seconds for QBF benchmarks}
\end{figure}

\section{Conclusion}

Operational Type Theory combines a dependently typed programming
language with a first-order theory of its untyped evaluation.  By
separating proofs and programs, contrary to the Curry-Howard
isomorphism, we free programs to include constructs like general
recursion which are problematic for proofs; and provide a principled
basis for proofs to reason about programs with type annotations
dropped, via untyped operational equality.  The robustness of \optt's
central design ideas is shown by its ability to accomodate extensions
like termination casts, as well as uniqueness and ownership
annotations.  Functional modeling based on the latter enables
verification of non-functional code.  The case study and empirical
evaluation of the incremental LF checker \textsc{Golfsock}, written in
\textsc{Guru}, demonstrates that the \optt\ methodology can be applied
to build efficient verified programs.

\textbf{Acknowledgements:} Thorsten Altenkirch for detailed comments
on an earlier draft; Daniel Tratos and Henry Li for additions to the
\textsc{Guru} standard library; and the NSF for support under award
CCF-0448275.

\bibliographystyle{plainnat}

%\nocite{SH80}
\bibliography{partiality,misc_logic,automated_reasoning,formal_methods,verification,lf,general,refinement,coop_dec_procs,cl,rewriting,theorem_provers,sat,program_analysis,software_engineering,specification,pl,stanford_group,hoas,semantic_programming,misc}



\end{document}
